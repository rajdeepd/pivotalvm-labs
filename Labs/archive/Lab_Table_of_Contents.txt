Table of contents for labs

HAWQ_DDL: HAWQ create tables exercise
  (a) Run the DDL SQL script for all the tables for the labs

HDFS_LOAD: Loading data into HDFS
  Use Hadoop command line utility to copy data files to HDFS

HBASE_HAWQ_LOAD: Data Loading: HBase, HAWQ
  (a) Load dimension table(s) into HBase using "importtsv" (data is in HDFS)
  (b) Load data into HAWQ using COPY (data is in DAS)
  (c) Load a HAWQ AO table using a SELECT from one of the GPXF tables defined

GPXF_PUSHDOWN: GPXF External Tables I -- predicate push-down
  (a) Review the DDL run in HAWQ_DDL
  (b) Run query involving HBase table ("customers_dim" table)
  (c) Check the value of the GUC "gpxf_enable_filter_pushdown" (using "show")
  (d) Toggle that to "off"
  (e) Rerun the query

HD_STREAM_ETL: Map/Reduce job for ETL
  Use Hadoop's Streaming API to transform data in HDFS

GPXF_STATS: GPXF External Tables II -- table statistics
  (a) SELECT relpages, reltuples FROM pg_class WHERE relname = 'table_name';
  (b) ANALYZE table_name;
  (c) SELECT relpages, reltuples FROM pg_class WHERE relname = 'table_name';

HIVE_VS_HAWQ: Hive DDL and query comparison
  (a) Run the Hive DDL to create Hive external tables against existing HDFS data
  (b) Run some queries against HAWQ and Hive versions of these tables

WORD_COUNT: Compile and run the classic map/reduce example
  Includes an Apache Ant build.xml file to help automate the compile, JAR, run
  iterations for experiments
  (a) Try running it as-is
  (b) Make a backup of the WordCount.java source file, edit it, and run the
      new version


